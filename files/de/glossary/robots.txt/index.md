---
title: Robots.txt
slug: Glossary/Robots.txt
l10n:
  sourceCommit: 530c1f54e63834411aa38789b1ac82e3831c4dfa
---

{{GlossarySidebar}}

`Robots.txt` ist eine Datei, die Ã¼blicherweise im Stammverzeichnis einer Website platziert wird. Sie entscheidet, ob [Crawler](/de/docs/Glossary/crawler) Zugriff auf die Website haben oder nicht.

Zum Beispiel kann der Webseiten-Administrator Crawler daran hindern, einen bestimmten Ordner (und alle darin enthaltenen Dateien) zu besuchen oder eine spezifische Datei zu crawlen, normalerweise um zu verhindern, dass diese Dateien von anderen Suchmaschinen indexiert werden.

## Siehe auch

- [Robots.txt](https://en.wikipedia.org/wiki/Robots.txt) auf Wikipedia
- <https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt>
- Standardspezifikation: [RFC9309](https://www.rfc-editor.org/rfc/rfc9309.html)
- <https://www.robotstxt.org/>
